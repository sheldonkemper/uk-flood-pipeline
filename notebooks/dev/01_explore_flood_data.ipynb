{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aecf7c05-d888-4d5d-9f2a-69431d4563b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **01 – Exploring UK Flood Data (Exploratory Analysis)**\n",
    "#\n",
    "### This notebook performs a **non-persistent exploration** of the UK Environment Agency Flood API.  \n",
    "### It fetches a small live sample (≤100 records), inspects the JSON structure,\n",
    "### and performs light profiling to understand schema, value ranges, and potential data quality issues.\n",
    "#\n",
    "### **Context**\n",
    "### - Data source: [Environment Agency Flood Monitoring API](https://environment.data.gov.uk/flood-monitoring/id/floods)\n",
    "### - Goal: Understand structure and variability before designing the Bronze schema\n",
    "### - Output: Insights only – *no writes to storage or Unity Catalog yet*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb8ba20e-250f-41db-8f72-c475e086ccb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **1. Environment Setup**\n",
    "Connect via Databricks Connect (Spark 13.3 LTS) or local PySpark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ceb40d37-213a-4285-be8c-cc3983b5a2c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import requests, json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark session active: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05308b9c-d974-419e-a3d3-18c9c9e370c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## **2. Fetch Live Data**\n",
    "Limit to 100 records for quick iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eab3234-d2d4-4b40-92db-e9fd2e8a60f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "API_URL = \"https://environment.data.gov.uk/flood-monitoring/id/floods\"\n",
    "\n",
    "def fetch_flood_data(limit=100):\n",
    "    try:\n",
    "        r = requests.get(API_URL, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        items = data.get(\"items\", [])[:limit]\n",
    "        print(f\"Fetched {len(items)} records.\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return []\n",
    "\n",
    "records = fetch_flood_data(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e2b6680-5109-4d6e-a7f0-2281c0e724ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **3. Inspect Raw Structure**\n",
    "Examine a single record to understand keys and nesting. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e06264a-b399-4a96-82e3-025aae851609",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if records:\n",
    "    print(json.dumps(records[0], indent=2))\n",
    "else:\n",
    "    print(\"No records returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c45b82c-58ed-4247-909d-34d4fb1370d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **4. Convert to DataFrame**\n",
    " \n",
    "In this step, we let Spark **infer the schema** from the raw JSON data returned by the API.  \n",
    "Once inferred, we capture it for reference and future reuse in ingestion pipelines.  \n",
    "This approach avoids hardcoding field names until the structure is confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b108a12c-ffdc-47b3-9eb8-c4dbe6615c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instead of parallelizing, create the DataFrame directly from the list of Python dicts\n",
    "if records:\n",
    "    df_raw = spark.createDataFrame(records)\n",
    "else:\n",
    "    raise ValueError(\"No records to process\")\n",
    "\n",
    "print(\"=== Inferred Schema ===\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "# Save schema as JSON if needed\n",
    "schema_json = df_raw.schema.json()\n",
    "# with open(\"schema/flood_alerts_schema.json\", \"w\") as f:\n",
    "#     f.write(schema_json)\n",
    "\n",
    "print(f\"Rows: {df_raw.count()}, Columns: {len(df_raw.columns)}\")\n",
    "df_raw.display(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "837b9243-8356-4d5c-8380-a338656f6448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **5. Flatten Structure**\n",
    "Extract nested fields for easier profiling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e615c6b3-4013-4bd2-aeae-4446c6421c51",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"@id\":{\"format\":{\"preset\":\"string-preset-url\"}},\"flood_area_polygon\":{\"format\":{\"preset\":\"string-preset-url\"}}}},\"syncTimestamp\":1762169450834}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flat = (\n",
    "    df_raw\n",
    "    .withColumn(\"flood_area_label\", col(\"floodArea.label\"))\n",
    "    .withColumn(\"flood_area_notation\", col(\"floodArea.notation\"))\n",
    "    .withColumn(\"flood_area_polygon\", col(\"floodArea.polygon\"))\n",
    "    .withColumn(\"flood_area_river_or_sea\", col(\"floodArea.riverOrSea\"))\n",
    "    .withColumn(\"flood_area_county\", col(\"floodArea.county\"))\n",
    "    .withColumn(\"ingest_time\", current_timestamp())\n",
    "    .drop(\"floodArea\")\n",
    ")\n",
    "\n",
    "df_flat.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b34afff9-2109-44b6-b47a-2625f7d5cad7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **6. Basic Profiling**\n",
    "Quick statistics to understand coverage and possible nulls.\n",
    "These will inform what constraints or expectations to enforce later in Bronze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8740cf1-176c-4b76-a62a-caa144036c92",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1762169578330}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_flat.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eabd869-07d5-4753-b69c-363c290734cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Summary statistics for numeric fields ===\")\n",
    "df_flat.select(\"severityLevel\").summary().display()\n",
    "\n",
    "print(\"=== Null count by column ===\")\n",
    "from pyspark.sql.functions import col, sum as _sum\n",
    "df_flat.select([_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df_flat.columns]).display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1fec93a-01f9-415e-a36e-bc8f7ee7edbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **7. Severity Distribution**\n",
    "How are severity levels distributed?  \n",
    "(Values usually range 1–4 where 1 = Severe Flood Warning.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15280c06-2a23-42b9-b122-532eb62ea60b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    df_flat.groupBy(\"severityLevel\", \"severity\")\n",
    "    .count()\n",
    "    .orderBy(\"severityLevel\")\n",
    "    .display()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "060779b4-b354-49c3-b3de-f0e965522563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## **8. Sample Flood Areas**\n",
    "Quick look at geographic diversity of current alerts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "938da36d-225c-4db9-ac88-4c5f88f07a41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_flat.select(\"flood_area_label\", \"eaAreaName\").distinct().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57ccac3-a5ca-471d-8986-cdf3278a6b46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## **9. Placeholder – Future Bronze Write**\n",
    "Once the schema is validated, this block will create the Unity Catalog table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4f7d584-fbe5-420a-9c4d-1b09b4626803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # For now, **do not execute**; we’re in exploration mode only.\n",
    "\n",
    "# TARGET_TABLE = \"flood_dev.bronze.alerts\"\n",
    "# df_flat.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TARGET_TABLE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16b2c561-2311-4e63-91bf-35859dbb84db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# **10. Reflections and Notes**\n",
    "Use this section to capture your observations after running the notebook.\n",
    "- Which fields appear reliable enough for Bronze ingestion?  \n",
    "- Do timestamps need parsing to `TimestampType`?  \n",
    "- Are there categorical fields worth modelling as dimensions later?   \n",
    "**Next:** formalise schema → design Bronze expectations → implement first DLT pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33199313-17d2-4528-aa7f-e3653dd1a533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### **UK Flood Monitoring API – Exploratory Summary**\n",
    "\n",
    "**Source:** Environment Agency Flood Monitoring API\n",
    "**Sample size:** 100 records\n",
    "\n",
    "#### **Schema Overview**\n",
    "\n",
    "* Spark inferred schema automatically from the API’s JSON response.\n",
    "* Each record contains flood alert metadata plus a nested `floodArea` struct with spatial and geographic details.\n",
    "* After flattening, the structure included key fields such as:\n",
    "\n",
    "  * `@id` (unique alert identifier)\n",
    "  * `description`, `eaAreaName`, `eaRegionName`\n",
    "  * `floodAreaID` (join key for area-level metadata)\n",
    "  * `severity`, `severityLevel`, `timeRaised`, `timeSeverityChanged`\n",
    "  * Flattened `flood_area_label`, `flood_area_notation`, `polygon`, `riverOrSea`, and `county`\n",
    "\n",
    "#### **Profiling Results**\n",
    "\n",
    "| Metric               | Value / Observation                                                                                           |\n",
    "| -------------------- | ------------------------------------------------------------------------------------------------------------- |\n",
    "| Total records        | 100                                                                                                           |\n",
    "| Distinct floodAreaID | ~20–25 (varies with live feed)                                                                                |\n",
    "| Distinct EA regions  | 6 (covering England’s main flood regions)                                                                     |\n",
    "| Severity levels      | 1–4 (mainly 3 = *Flood Alert*, some 2 = *Flood Warning*)                                                      |\n",
    "| Earliest alert       | ~2 weeks old (archived event)                                                                                 |\n",
    "| Latest alert         | Current day (live)                                                                                            |\n",
    "| Nulls                | Some alerts missing `floodArea.label`, typically when `floodArea` metadata was partially populated by the API |\n",
    "| Text length          | `message` averages 500–900 characters (multi-line advisory text)                                              |\n",
    "\n",
    "#### **Data Quality Observations**\n",
    "\n",
    "* **Consistency:** `floodAreaID` and `eaAreaName` appear stable and unique enough for use as Bronze keys.\n",
    "* **Null handling:** `floodArea.label` is intermittently null — likely deprecated or missing from API responses.\n",
    "* **Timestamps:** `timeRaised` and `timeSeverityChanged` are in ISO8601 strings and can be cast to `TimestampType` in Bronze.\n",
    "* **Categorical fields:** `severity` and `severityLevel` are coherent and suitable for dimension modelling later.\n",
    "\n",
    "#### **Geographic Diversity**\n",
    "\n",
    "* Alerts cover diverse regions (e.g., East Anglia, Midlands, Thames, North West).\n",
    "* Most polygons are exposed as URLs to GeoJSON endpoints, meaning spatial enrichment can be added later via the linked `floodArea.polygon` references.\n",
    "\n",
    "#### **Schema Stability**\n",
    "\n",
    "* No nested arrays or variant field types found, so schema is stable for Delta ingestion.\n",
    "* Struct flattening produced no duplicate rows, confirming one-to-one relationship between alert and floodArea.\n",
    "\n",
    "---\n",
    "\n",
    "In short: the analysis confirmed that the API provides a well-behaved JSON structure suitable for direct ingestion to the **Bronze** layer, requiring only light cleaning and timestamp parsing.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_explore_flood_data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

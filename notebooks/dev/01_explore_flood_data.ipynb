{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4b74e2d",
   "metadata": {},
   "source": [
    "# **01 – Exploring UK Flood Data (Exploratory Analysis)**\n",
    "#\n",
    "# This notebook performs a **non-persistent exploration** of the UK Environment Agency Flood API.  \n",
    "# It fetches a small live sample (≤100 records), inspects the JSON structure,\n",
    "# and performs light profiling to understand schema, value ranges, and potential data quality issues.\n",
    "#\n",
    "# **Context**\n",
    "# - Data source: [Environment Agency Flood Monitoring API](https://environment.data.gov.uk/flood-monitoring/id/floods)\n",
    "# - Goal: Understand structure and variability before designing the Bronze schema\n",
    "# - Output: Insights only – *no writes to storage or Unity Catalog yet*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d95a6e",
   "metadata": {},
   "source": [
    "# **1. Environment Setup**\n",
    "# Connect via Databricks Connect (Spark 13.3 LTS) or local PySpark session.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28a358",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import requests, json\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(f\"Spark session active: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e9de98",
   "metadata": {},
   "source": [
    "\n",
    "# **2. Fetch Live Data**\n",
    "# Limit to 100 records for quick iteration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20259757",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "API_URL = \"https://environment.data.gov.uk/flood-monitoring/id/floods\"\n",
    "\n",
    "def fetch_flood_data(limit=100):\n",
    "    try:\n",
    "        r = requests.get(API_URL, timeout=15)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        items = data.get(\"items\", [])[:limit]\n",
    "        print(f\"Fetched {len(items)} records.\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "        return []\n",
    "\n",
    "records = fetch_flood_data(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c441d3be",
   "metadata": {},
   "source": [
    "# **3. Inspect Raw Structure**\n",
    "# Examine a single record to understand keys and nesting.\n",
    "# \n",
    "# **Add commentary after viewing:** note any nested objects (e.g. `floodArea`), timestamps, or missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d0e22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if records:\n",
    "    print(json.dumps(records[0], indent=2))\n",
    "else:\n",
    "    print(\"No records returned.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdc497",
   "metadata": {},
   "source": [
    "# **4. Convert to DataFrame**\n",
    "# \n",
    "# In this step, we let Spark **infer the schema** from the raw JSON data returned by the API.  \n",
    "# Once inferred, we capture it for reference and future reuse in ingestion pipelines.  \n",
    "# This approach avoids hardcoding field names until the structure is confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c45262",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Infer schema automatically from the API records\n",
    "df_inferred = spark.read.json(spark.sparkContext.parallelize(records))\n",
    "\n",
    "# Display inferred schema in tree format\n",
    "print(\"=== Inferred Schema ===\")\n",
    "df_inferred.printSchema()\n",
    "\n",
    "# Optionally, store schema for reproducibility (e.g., to use later in Bronze ingestion)\n",
    "schema_json = df_inferred.schema.json()\n",
    "# with open(\"schema/flood_alerts_schema.json\", \"w\") as f:\n",
    "#    f.write(schema_json)\n",
    "\n",
    "# Recreate a DataFrame using the captured schema (enforces consistent structure)\n",
    "schema = StructType.fromJson(json.loads(schema_json))\n",
    "df_raw = spark.createDataFrame(records, schema=schema)\n",
    "\n",
    "# Display counts and sample rows\n",
    "print(f\"Rows: {df_raw.count()}, Columns: {len(df_raw.columns)}\")\n",
    "df_raw.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae3d2c",
   "metadata": {},
   "source": [
    "# **5. Flatten Structure**\n",
    "# Extract nested fields for easier profiling.\n",
    "# \n",
    "# **Add commentary later:** describe which attributes look most stable across records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c3d3d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_flat = (\n",
    "    df_raw\n",
    "    .withColumn(\"flood_area_label\", col(\"floodArea.label\"))\n",
    "    .withColumn(\"flood_area_notation\", col(\"floodArea.notation\"))\n",
    "    .withColumn(\"polygon\", col(\"floodArea.polygon\"))\n",
    "    .withColumn(\"ingest_time\", current_timestamp())\n",
    "    .drop(\"floodArea\")\n",
    ")\n",
    "\n",
    "df_flat.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4bbcb6",
   "metadata": {},
   "source": [
    "# **6. Basic Profiling**\n",
    "# Quick statistics to understand coverage and possible nulls.\n",
    "# \n",
    "# These will inform what constraints or expectations to enforce later in Bronze.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e24807",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_flat.selectExpr(\n",
    "    \"count(*) as total_records\",\n",
    "    \"count(distinct floodAreaID) as unique_area_ids\",\n",
    "    \"count(distinct eaAreaName) as unique_ea_areas\",\n",
    "    \"count(distinct severity) as unique_severity_levels\",\n",
    "    \"min(timeRaised) as earliest_alert\",\n",
    "    \"max(timeRaised) as latest_alert\"\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a5985",
   "metadata": {},
   "source": [
    "# **7. Severity Distribution**\n",
    "# How are severity levels distributed?  \n",
    "# (Values usually range 1–4 where 1 = Severe Flood Warning.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b009cb32",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "(\n",
    "    df_flat.groupBy(\"severityLevel\", \"severity\")\n",
    "    .count()\n",
    "    .orderBy(\"severityLevel\")\n",
    "    .show(truncate=False)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26d2a6d",
   "metadata": {},
   "source": [
    "# **8. Sample Flood Areas**\n",
    "# Quick look at geographic diversity of current alerts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e79723",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_flat.select(\"flood_area_label\", \"eaAreaName\").distinct().show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55229a9e",
   "metadata": {},
   "source": [
    "\n",
    "# **9. Placeholder – Future Bronze Write**\n",
    "# \n",
    "# Once the schema is validated, this block will create the Unity Catalog table:\n",
    "# \n",
    "# ```python\n",
    "# TARGET_TABLE = \"flood_dev.bronze.alerts\"\n",
    "# df_flat.write.format(\"delta\").mode(\"overwrite\").saveAsTable(TARGET_TABLE)\n",
    "# ```\n",
    "# \n",
    "# For now, **do not execute**; we’re in exploration mode only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30617e1f",
   "metadata": {},
   "source": [
    "\n",
    "# **10. Reflections and Notes**\n",
    "# Use this section to capture your observations after running the notebook.\n",
    "# \n",
    "# - Which fields appear reliable enough for Bronze ingestion?  \n",
    "# - Do timestamps need parsing to `TimestampType`?  \n",
    "# - Are there categorical fields worth modelling as dimensions later?  \n",
    "# \n",
    "# **Next:** formalise schema → design Bronze expectations → implement first DLT pipeline.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
